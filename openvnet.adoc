= OpenVNetで本格的な仮想ネットワーク
:imagesdir: images/openvnet

// TODO ほかの章と同じく、リード文を2,3行ほど追加する

== OpenVNetとは

OpenVNetはOpenFlowで仮想ネットワークを構築するためのOSSです。Tremaを使ってあらゆるパケットの挙動を自由に制御することで、既存のネットワーク上にあたかもユーザ専用のネットワークがあるかのような環境を作り出すことができます。開発はWSF(Wakame Software Foundation)が中心となっており、筆者の一人である山崎の所属する株式会社あくしゅの開発者がメインコミッターを務めています。

=== 仮想ネットワークを構築するエッジオーバレイ型のOSS

OpenVNetによるネットワーク仮想化の特長は「エッジオーバーレイ型」である点です。既存のネットワーク内にOpenFlowスイッチを制御するエッジサーバを設置し、ここで全てのパケットを制御することで、あたかも独立したネットワークがあるかのように通信させます。

[[edge_network_virtualization]]
image::edge_network_virtualization.png[caption="図17-1",title="エッジオーバーレイによるネットワーク仮想化"]

エッジサーバの主な仕事は、物理ネットワークと仮想ネットワーク間でのパケットの相互書き換えです。

1. サーバから仮想ネットワークに送信したパケットは、エッジが制御するOpenFlowスイッチで物理ネットワークを通るように書き換え、宛先のサーバへ送出する
2. 宛先サーバに届く直前のスイッチで逆の書き換えを行う。つまり、物理ネットワークを通ってきたパケットを仮想ネットワーク内のパケットに見えるように書き換える

こうしたエッジによるパケットの書き換えはサーバからは見えません。OpenVNetの作り出した仮想ネットワークが、サーバからは物理ネットワークであるように見えます。

[[edge_translation]]
image::edge_translation.png[caption="図17-2",title="エッジによるパケットの書き換え"]

OpenVNetのもう一つの大きな特長は、OpenFlow化されていない既存のネットワーク上で動作することです。たとえば <<sliceable_switch,第16章>> で紹介したスライサブルスイッチはネットワークスイッチがすべてOpenFlowに対応しているという前提がありました。一方OpenVNetでは、この制御をエッジサーバ上に起動したエッジスイッチだけで行います。こうすることで、既に構築されたネットワークの上で仮想ネットワークを実現できます。

// TODO: この説明は高宮が勝手に追加しましたが、合っていますか？

// TODO: 図が欲しい。物理ネットワーク(L2, L3, VPNでDB跨ぎ)に、仮想ネットワークをマッピングする図 = 基本的な考え方として理解できるもの

== エッジオーバーレイ仮想ネットワークの利点

OpenVNetのようなエッジオーバーレイ型は、次の2つの場面で特に威力を発揮します。

1. 既存データセンターの活用
2. ダウンサイジング

=== 既存データセンターの活用

最小の変更だけで既存データセンター上に仮想ネットワークサービスを構築できます。エッジオーバーレイによるネットワーク仮想化はほぼエッジノードの追加だけでできます。このため、既存の物理ネットワークの敷設や再設定をできるだけ抑えながら、その上に新しく仮想ネットワークを構築して提供できるようになります。

=== ダウンサイジング

ネットワークの仮想化によりネットワークの収容効率を改善できます。たとえばサーバーの世界では、場所を取っていた大量の古い物理サーバーを仮想化し少数の物理サーバーに大量に詰め込み、さらにワークロードのばらつきを利用することで、収容効率が大幅に向上しました。仮想ネットワークでも、これと同じダウンサイジングが可能です。

近年のネットワーク帯域向上により「詰め込み」がますます現実的になってきました。たとえば10Gbpsの物理ネットワークには、単純計算すれば10Mbpsの仮想ネットワークを100個収容できます。さらに、それぞれのネットワークはいつも10Mbps使いきっているわけではありませんから、ばらつきを考慮し効率的に配置すれば、より多くを収容できます。これによって、古い大量の物理スイッチを仮想ネットワーク化することで一掃できます。

== OpenVNetの主な機能

OpenVNetが提供するたくさんの機能のうち、代表的なものは次の5つです。

1. 仮想L2セグメントの作成
2. 仮想L2セグメント間のルーティング
3. セキュリティグループ
4. DHCPとDNS
5. 既存ネットワークと仮想ネットワークの接続

=== 仮想L2セグメントの作成

ノードのNICが、あたかも同じスイッチのL2セグメントに接続されたように見える機能です。例えば、ロードバランサー配下のWebサーバに対するセグメント、Webサーバからデータベースサーバに対するセグメントなど、必要となるL2セグメントを任意に作成することができ、Webサーバやデータベースサーバと言ったノードのNICと、自由に接続することができます。

// TODO: 以下、それぞれの項目ごとに簡単な図がほしい

物理ネットワーク上に、同一のセグメントを複数作っても問題はありません。仮想ネットワークの世界ではそれらセグメントは全て適切に識別され、パケットの輻輳が起こらないように設計されています。

=== 仮想L2セグメント間のルーティング

作成した仮想L2セグメント間にL3の制御を入れて自由にルーティングできます。これは、ルータを仮想的に配置するようなものです。

// TODO: 簡単な図がほしい

ただし、Vyattaのような仮想ルータを実際に配置しているわけではなく、エッジ側のスイッチのフローによって静的なルーティングを実現されています。ノード間のパケットは余計なネットワーク経路を辿らず、エッジスイッチ間をピアで最適な通信をします。もし動的なルーティングの機能が必要であれば、Vyattaを内蔵した仮想マシンを起動し、NICを複数持たせて、仮想L2セグメントをルーティングさせることもできます。

=== セキュリティグループ

エッジスイッチは各ノードのトラフィック全ての関所でもあります。セキュリティグループは、この関所に、パケットの受け入れ許可ルールを指定し、ファイアウォールとして機能させるものです。

// TODO: 簡単な図がほしい

ただし、全てのノードをひとつずつ指定していく煩雑さを解消するため、論理名を付けたグルーピングと、グループ間の通信許可を指定することが出来るようになっています。特にグループ間の通信許可の場合は、グループに属するノードに変更があれば、相対するグループの設定にも動的に影響が及びます。

=== DHCPとDNS

DHCPやDNSなどのサービスをエッジノードで処理できます。

// TODO: 簡単な図がほしい

本来は、ネットワーク上にDHCPサーバを設置し、そのサーバがDHCPのディスカバリ(IPアドレスの問い合わせパケット)に応答することになっています。しかし、わざわざDHCPサーバまで到達させずとも、応答すべきパケットが自明である場合は、エッジスイッチでパケットを生成して折り返してしまうことができます。ノードに割り振られるIPアドレスが自明である場合に、この機能が利用できます。

=== 既存ネットワークと仮想ネットワークの接続

仮想ネットワークの世界の境界(VNetEdgeと呼びます)を外界と接続するための橋渡し方式を提供します。

// TODO: 簡単な図がほしい

仮想ネットワークは、ある意味閉じたネットワークです。物理ネットワーク上にオーバレイされた、新しい仮想のL2セグメントですので、既存のネットワークからどのようにしてパケットを送受信しあうかも重要なポイントになります。VNetEdgeで受け取ったパケットを読み取り、仮想ネットワークへ流し込むルールを決めるトランスレーションと言う方法があります。トランスレーションのルールはシンプルで、パケットに記載されている情報を元に、指定された仮想ネットワークへとマッピングするものです。例えば、Tagged VLANのIDと、任意の仮想ネットワークをマッピングしてみたり、IPアドレスと仮想ネットワークのIPアドレスをマッピングしNATのようにしてみたりすることができます。

== アーキテクチャと動作

OpenVNetのアーキテクチャは非常にシンプルです。vnmgr(Virtual Network Manager)が、ネットワーク全体の構造を保持するデータベースと、Web APIを提供します。データベースから、仮想ネットワークのあるべき設定を割り出したvnmgrは、分散するvna(Virtual Network Agent)に、担当するスイッチに対しフローを設定するよう指示するのです。vnaは、接続されたスイッチ(Open vSwtich)に対し、OpenFlow仕様を含むフローの設定と、OpenFlow Controllerとして、DHCPなど反応すべきパケットに対する処理を任されています。


=== フロー制御

// TODO
- https://github.com/axsh/openvnet/wiki/FlowTable

== 使ってみる

// TODO
- https://github.com/axsh/openvnet/blob/master/docs/InstallGuide.md

=== CLIで操作してみる

// TODO

=== フローの変化を見る

// TODO

=== 疎通確認をする

// TODO

== OpenVNetの活用例

OpenVNetはすでに活用が始まっています。たとえば、京セラコミュニケーションシステムやTIS株式会社にて、OpenFlowの実案件活用や仮想ネットワークの実証実験などを行っており、OpenVNetのテクノロジが活躍しています。

=== IaaS基盤でネットワーク管理をする

WSFでは株式会社あくしゅが筆頭となり、Wakame-vdcと言うIaaS基盤を開発しており、多くのデータセンタで商用利用が始まっています。Wakame-vdcは、データセンタ内部のコンピューティング資源を、動的にマルチテナント化するソフトウェアです。公表可能なものだけでも、すでにいくつもの企業や研究機関で商業化や実用化が進んでいます。

- 国立情報学研究所 (NII): 分散処理の実証実験、クラウド教育教材として活用
- 九州電力: 大規模データの分散処理基盤として
- NTT PCコミュニケーションズ: パブリッククラウド WebARENA VPSクラウド
- 京セラコミュニケーションシステム: パブリッククラウド GreenOffice Unified Cloud
- TIS株式会社: Dockerでの活用、クラウドを跨ったL2延伸の共同実証実験

マルチテナントをするためには、仮想化の技術が重要になります。サーバだけでなく、ネットワークも含めて、仮想化を実現しなければなりません。特に後者について、2012年の始めに、Wakame-vdcはTremaを利用して、仮想ネットワークの技術を内蔵していました。これが後の2013年の秋に分離されて、OpenVNetとしてスピンアウト、独立しました。

=== 分散するDockerを仮想L2で連結する

次に２つ目の活用例として、複数のサーバ上に分散するDockerコンテナをOpenVNetを用いた仮想L2セグメントで連結する方法をみていきましょう。

これは2015年のはじめに、TIS株式会社が仮想ネットワークとコンテナ技術の実験を行った事例となります。

=== Dockerのネットワーク

Docker footnote:[Dockerの詳細は、Dockerの公式ドキュメント(https://docs.docker.com/)を参照ください] とは、dotCloud社（現Docker社）が自社のパブリックPaaSを実現するために開発した技術をOSS化したものです。アプリケーションの実行環境を容易に素早く、かつ他の影響を受けないようにして立ち上げるために、Dockerは様々なLinuxの技術を用いて「他から隔離された環境（＝コンテナ） 」を作り出します。

Dockerは様々なリソースを隔離しますが、ネットワークもその隔離すべきリソースの一つです。そのためDockerは、Network Namespaceや仮想NIC等の技術を用いて、ホストとなるLinuxサーバ上に他から隔離された内部ネットワークを構成します。ただしそのままではサーバの外部と通信ができませんので、Dockerは通常、IPマスカレードとポートフォワードをiptablesに設定することで外部ネットワークと連携できるようにします。

[[docker_network]]
image::docker_network.png[caption="図17-1",title="Dockerのネットワーク"]

単独のサーバ内でDockerを利用するだけならこの方式で良いのですが、複数のサーバでDockerを動作させたい場合には問題が生じます。Dockerコンテナが所属するネットワークはサーバ内に閉じていますので、異なるサーバで動作しているDockerコンテナ同士が、そのDockerコンテナに付与されたIPアドレスで通信することができないのです。

この問題を解決するために、 coreos/flannel footnote:[https://github.com/coreos/flannel] や weaveworks/weave footnote:[https://github.com/weaveworks/weave] 等の様々なDockerネットワーキングツールが公開されています。また2015年10月にリリースされたDocker v1.9からは、Docker自身が複数サーバを跨った仮想ネットワークを構成できるようになりました。

しかしこの実験を行った時点ではまだ、Docker自身は仮想ネットワークを構成する機能を持っていませんでした。また coreos/flannel や weaveworks/weave といったツールには、セキュリティグループのようなOpenVNetが持つ高度なネットワーク機能がありませんでした。そこで本実験では、OpenVNetを用いて敷設した仮想L2セグメントにDockerコンテナを所属させることで、サーバを跨ったDockerコンテナ間がシームレスに通信できること、及びDockerのネットワークにセキュリティグループのような高度なネットワーク機能を付与できることを確認しました。

=== Docker+OpenVNet

同一サブネット内のサーバ2台と、ルータを挟んだ別のサブネットにあるサーバ1台の、合計3つのサーバ上でDockerコンテナを動作させ、それらをOpenVNetを用いて敷設した仮想L2セグメントに所属させてみましょう。

まずは各サーバ上でDockerコンテナを立ち上げた後、DockerコンテナのNetwork Namespaceとopen vSwitchにvethペアを放り込みます。この際、後からOpenVNetに設定できるように、Dockerコンテナ側のvethのMACアドレスとIPアドレスを明示的に指定しておきます。

次にOpenVNetを設定します。Open vSwitchのdatapathIDを重複しないように指定し、各サーバが所属する物理ネットワークとOpenVNetが敷設する仮想L2セグメントをOpenVNetに定義します（この際、異なる物理サブネットに存在するサーバ間はGREトンネルが自動的に敷設されます）。各サーバの物理NICと、Dockerコンテナにvethペアとして放り込んだ仮想NICの情報、及びセキュリティグループをOpenVNetに設定した後に、各仮想NICにセキュリティグループを割り当て、OpenVNet上に仮想ルータを構成して物理ネットワークと仮想L2セグメント間のルーティングを定義しましょう。

最後に各サーバとDockerコンテナにスタティックルートを設定すれば、OpenVNetを用いたDockerネットワーキングが完成します。各サーバ上のDockerコンテナはOpenVNetが敷設した同じ仮想L2セグメントに所属していますので、異なるサーバのDockerコンテナへそのIPアドレスを用いて通信することが可能となります。またセキュリティグループの設定に従い、到達すべきでないパケットはOpenVNetがDROPするため、個々のDockerコンテナにパケットフィルタルールを定義する必要が無くなります。

image::docker_openvnet_1.png[caption="図17-2",title="OpenVNetを用いたDockerネットワーキング"]

なお、ここで説明した手順を実際に実行し動作させるツールキットを、walfisch footnote:[https://github.com/tech-sketch/walfisch] というオープンソースソフトウェアとして公開しています。実際に実行したコマンドが標準出力に表示されますので、OpenVNetを用いたDockerネットワーキングに興味がある方は一度動作させてみると良いでしょう。

=== 分散するデータセンタ間を仮想L2で連結する

最後に、 複数のデータセンタ間を跨って任意の仮想L2ネットワークを構成する例を見てみましょう。
この例は、2014年度にTIS株式会社と株式会社あくしゅが協力し、各IaaSやオンプレミスのネットワーク機能に依存しないネットワーク制御について、OpenVNetを活用して共同検証を行ったものです。

現存するパブリックIaaSの持つネットワーク機能は、それぞれ大きく利用方法や特徴が異なっています。このため、パブリックIaaSの利用者はそれらに強く依存したシステム設計を行う必要があります。しかし、OpenVNetを利用することで、パブリックIaaSのネットワーク機能に依存せず、複数のパブリックあるいはプライベートIaaSに跨った仮想的なL2ネットワークを構成することが可能となるため、IaaS間の段階的なシステム移行の実現性を高めることができます。

=== プライベートIaaSとパブリックIaaSの連結例

それでは、プライベートIaaSとパブリックIaaSのL2ネットワークを仮想的に連結する構成例をみてみましょう。

OpenVNetは独立して動作することができますが、本来は仮想データセンタを構築するOSSであるwakame-vdc footnote:[https://github.com/axsh/wakame-vdc] のネットワーク機能としてスピンアウトしたソフトウェアであるため、プライベートIaaSとしてwakame-vdc、パブリックIaaSとしてAmazon Web Servicesを利用するケースを想定します。

[[narukozaka_tools]]
image::narukozaka_tools.png[caption="図17-4",title="プライベートIaaSとパブリックIaaSの連結構成"]

OpenVNetは、フローによってOpenVNetの仮想ネットワークと外部のネットワークの間をシームレスに接続するVNetEdge機能を持っています。

この構成例では、仮想ネットワークIDとVLAN IDの変換規則をOpenVNetに登録しておくことで、wakame-vdcの仮想ネットワークと、Amazon Web ServicesのVirtual Private Cloudで構築されたネットワークの間を流れるパケットがVNetEdgeのOpen vSwitchを通過する際に、この２つのネットワークが同一のL2ネットワークであるかのようにパケット転送を制御します。

このツールキットはOSSとして公開しており footnote:[https://github.com/cloudconductor-incubator/narukozaka-tools]、この他にも多くの機能を持ちます。

* IaaSのインスタンスイメージの作成と起動
* IaaSのインスタンスにインストールするミドルウェアの自動設定
* IaaSのネットワーク上に、VNetEdgeをスイッチとしたスター型のネットワークトポロジを構築する機能
* wakame-vdcとパブリックIaaSの間を自動的にトンネリングする機能

またセキュリティの案件に応じ、wakame-vdc側のインスタンスとIaaS側のVNetEdge間のGREトンネリングを暗号化するといった、柔軟な対応も可能です。

== なぜTremaを採用したのか

// TODO ここは少し削りましょう。本全体のトーンとして、Trema開発者による解説という視点で統一書いています。以下の部分だけユーザ視点が混ざっているので、Trema開発者側の視点に書き直す必要があります。

Tremaの最大の魅力は、数々あるのですが、大まかにまとめると以下の通りです。

1. 優れた設計がある: フレームワークとして最小限のコードで最大限の効果を得られる
2. コミュニティが機能している: コードを評価でき、貢献が適切に反映されている
3. 言語の親和性がある: OpenVNetはTremaと同じRubyで組まれている

まず、OpenVNetがOpenFlowを使っていく方針を出した際、様々なツールキットやフレームワークが出ており、いくつか調査をしました。その中で、Tremaは当初より、利用する側から見た設計が、非常に合理的で洗練されており、やりたい事に対していつでも最短のコードで目的に辿り着けるようになっていました。

また、今でこそ十分な機能がありますが、当初はまだ機能が足りない部分もあり、そこはコードをOpenVNet側からコミットして貢献することもできました。オンラインだけでなく、オフラインのコミュニティもOpenVNetのプロジェクトからは魅力的でした。次に書こうとしているコードの相談などもその場で可能なのです。コードは双方にとって有益であれば採用され、お互いにソフトウェアとして成長していくことができ、まさにバザール式の開発が機能しています。

Tremaは、OpenVNetにとって、あらゆる面から大きなアドバンテージのある選択でした。ソフトウェアは、それを使う人が育てていくことで、より良い物になっていきます。貢献の仕方は様々です。一番簡単なところでは、下記のような方法があるでしょう。

- GitHub上でWatchやForkをしてみる
- コードをダウンロードして使ってみる
- 思うところや、成果をブログを書いて公表してみる
- 既存ドキュメントの英訳や日本語訳をする
- 足りないドキュメントがあれば加筆や新規執筆をする
- GitHubのIssue機能を通じて、バグ報告をしてみる
- バグを修正するパッチの送付をしてみる
- 機能の追加をして提案をしてみる

上記を例に、自分の能力にあった貢献の仕方で、Tremaの世界を共に大きくしていくことができます。この素晴らしい取り組みに、ぜひ皆様ご参加ください。お待ちしております。
