= OpenVNetで本格的な仮想ネットワーク
:imagesdir: images/openvnet

仮想ネットワークの機能を、データセンター規模で本格的に利用することを目指して、分散するOpenFlowスイッチ群をOpenVNetを使ってコントロールしてみましょう。

== OpenVNetとは

OpenVNetはOpenFlowで仮想ネットワークを構築するためのフリーソフトウェアです。Tremaを使ってあらゆるパケットの挙動を自由に制御することで、既存のネットワーク上にあたかもユーザ専用のネットワークがあるかのような環境を作り出すことができます。開発はWSF(Wakame Software Foundation)が中心となっており、筆者の一人である山崎の所属する株式会社あくしゅの開発者がメインコミッターを務めています。ソフトウェアライセンスにはLGPLv3を採用し、組織の枠を越えたバザール式の開発がなされています。

OpenVNetは、もともとWSFの中にあった別プロジェクトから、ネットワークに関する技術だけをスピンオフして誕生したものです。スピンオフする元となったプロジェクトでは、Wakame-vdcと言う名前で、2009年からデータセンター全体を仮想化するためのソフトウェアを開発してきました。Wakame-vdcは、データセンタ内部のコンピューティング資源を、動的にマルチテナント化できます。すでにいくつもの企業や研究機関で商業化や実用化が進んでいます。下記に、現時点で公表可能なものだけを挙げます。

- 国立情報学研究所 (NII): 分散処理の実証実験、クラウド教育教材として活用
- 九州電力: 大規模データの分散処理基盤として
- NTT PCコミュニケーションズ: パブリッククラウド WebARENA VPSクラウド
- 京セラコミュニケーションシステム: パブリッククラウド GreenOffice Unified Cloud
- TIS株式会社: Dockerでの活用、クラウドを跨ったL2延伸の共同実証実験

マルチテナントをするためには、仮想化の技術が重要になります。サーバだけでなく、ネットワークも含めて、仮想化を実現しなければなりません。特に後者について、2012年の始めに、Tremaを用いてWakame-vdcへ仮想ネットワークの技術を実装しました。仮想ネットワークの技術だけを利用したいユーザ様のために、この部分の実装が後の2013年の秋にそのまま分離されて、OpenVNetとしてスピンアウト、独立しました。

=== 仮想ネットワークを構築するエッジオーバレイ型アーキテクチャ

OpenVNetによるネットワーク仮想化の特長は「エッジオーバーレイ型」である点です。仮想マシンのような通信を必要とする末端となるものと既存のネットワークとの間にOpenFlowスイッチを設置し、ここで全てのパケットを制御することで、あたかも独立したネットワークがあるかのように通信させます。このように、エッジオーバレイとは、末端に近いところで、いくつものネットワークを重ねて自由に定義するモデルを言います。また、この場所に存在するスイッチを一般に、エッジスイッチと呼びます。

[[edge_network_virtualization]]
image::edge_network_virtualization.png[caption="図17-1",title="エッジオーバーレイによるネットワーク仮想化"]

エッジスイッチの主な仕事は、物理ネットワークと仮想ネットワーク間でのパケットの相互書き換えです。

1. 仮想マシンから仮想ネットワークに送信したパケットは、エッジスイッチが物理ネットワークを通るように書き換え、宛先のサーバへ送出する
2. 宛先のサーバに届く直前のエッジスイッチで逆の書き換えを行う。つまり、物理ネットワークを通ってきたパケットを仮想ネットワーク内のパケットに見えるように書き換える

こうしたエッジスイッチによるパケットの書き換えはサーバからは見えません。OpenVNetの作り出した仮想ネットワークが、仮想マシンからは物理ネットワークであるように見えます。

[[edge_translation]]
image::edge_translation.png[caption="図17-2",title="エッジによるパケットの書き換え"]

OpenVNetのもう一つの大きな特長は、OpenFlow化されていない既存のネットワーク上で動作することです。たとえば <<sliceable_switch,第17章「ネットワークを仮想化する」>> で紹介したスライサブルスイッチには、ネットワークスイッチがすべてOpenFlowに対応しているという前提がありました。一方OpenVNetでは、この制御を物理サーバ上に起動したエッジスイッチだけで行います。こうすることで、既に構築されたネットワークの上で仮想ネットワークを実現できます。

// TODO: この説明は高宮が勝手に追加しましたが、合っていますか？
// あってます (山崎)

// TODO: 図が欲しい。物理ネットワーク(L2, L3, VPNでDB跨ぎ)に、仮想ネットワークをマッピングする図 = 基本的な考え方として理解できるもの

== エッジオーバーレイ仮想ネットワークの利点

OpenVNetのようなエッジオーバーレイ型は、次の2つの場面で特に威力を発揮します。

1. 既存データセンターの活用
2. ダウンサイジング

=== 既存データセンターの活用

最小の変更だけで既存データセンター上に仮想ネットワークサービスを構築できます。エッジオーバーレイによるネットワーク仮想化はほぼ物理サーバの追加だけでできます。このため、既存の物理ネットワークの敷設や再設定をできるだけ抑えながら、その上に新しく仮想ネットワークを構築して提供できるようになります。

=== ダウンサイジング

ネットワークの仮想化によりネットワークの収容効率を改善できます。たとえばサーバーの世界では、場所を取っていた大量の古い物理サーバーを仮想化し少数の物理サーバーに大量に詰め込み、さらにワークロードのばらつきを利用することで、収容効率が大幅に向上しました。仮想ネットワークでも、これと同じダウンサイジングが可能です。

近年のネットワーク帯域向上により「詰め込み」がますます現実的になってきました。たとえば10Gbpsの物理ネットワークには、単純計算すれば10Mbpsの仮想ネットワークを100個収容できます。さらに、それぞれのネットワークはいつも10Mbps使いきっているわけではありませんから、ばらつきを考慮し効率的に配置すれば、より多くを収容できます。これによって、古い大量の物理スイッチを仮想ネットワーク化することで一掃できます。

== 全体アーキテクチャ

OpenVNetのアーキテクチャは非常にシンプルです。vnmgr(Virtual Network Manager)が、ネットワーク全体の構造を保持するデータベースと、Web APIを提供します。データベースから、仮想ネットワークのあるべき設定を割り出したvnmgrは、分散するvna(Virtual Network Agent)に、担当するエッジスイッチに対しフローを設定するよう指示するのです。vnaは、接続されたスイッチ(Open vSwtich)に対し、OpenFlow仕様を含むフローの設定と、OpenFlow Controllerとして、DHCPなど反応すべきパケットに対する処理を任されています。vna内部で、OpenFlow Controllerの機能を実現するために、Tremaを使っています。

[[openvnet_architecture]]
image::openvnet_architecture.png[caption="図xx-y",title="OpenVNetの全体アーキテクチャ"]

vnctlは、OpenVNet全体を操作するためのコマンドラインインタフェースです。vnctlの引数に与えられたものを解釈して、vnmgrのWeb APIへ通信します。vnmgrは分散するvnaとの通信に、メッセージキューを利用することで、大規模なユースケースに対応する他、vnaが適切なタイミングでOpenFlow Switchと対話できるように設計されています。
現在、サポートしているソフトウェアセットは下記の通りです。

|===
| コンポーネント名称 | サポートしているソフトウェア名称 | URL

| OpenFlow Switch | Open vSwitch | http://openvswitch.org/
| Database | MySQL | http://www.mysql.com/
| Message Queue | ZeroMQ & Redis | http://zeromq.org/ http://redis.io/
|===

== OpenVNetの主な機能

OpenVNetが提供するたくさんの機能のうち、代表的なものは次の5つです。

1. 仮想ネットワークの作成
2. 仮想ネットワーク間のルーティング
3. セキュリティグループ
4. DHCPとDNS
5. 既存ネットワークと仮想ネットワークの接続

=== 仮想ネットワークの作成

仮想マシンのネットワークインタフェースが、あたかも同じスイッチに接続されたように見える機能です。例えば、ロードバランサー配下のWebサーバに対するスイッチ、Webサーバからデータベースサーバに対するスイッチなど、必要となるスイッチを任意に作成することができ、Webサーバやデータベースサーバと言った仮想マシンのネットワークインタフェースを、自由に接続することができます。

// TODO: 以下、それぞれの項目ごとに簡単な図がほしい

物理ネットワーク上にある物理スイッチに、同じIPアドレスを用いる仮想ネットワークを複数作っても問題はありません。エッジスイッチではそれら仮想ネットワークは全て適切に識別され、パケットの輻輳が起こらないように設計されています。

=== 仮想ネットワーク間のルーティング

作成した２つ以上の仮想ネットワークの間を自由にルーティングできます。これは、ルータを仮想的に配置するようなものです。

[[route_between_vnets]]
image::route_between_vnets.png[caption="図xx-y",title="仮想ネットワーク間のルーティング"]

ただし、Vyattaのような仮想ルータを実際に配置しているわけではなく、エッジスイッチのフローによって静的なルーティングを実現しています。仮想マシン間のパケットは余計なネットワーク経路を辿らず、エッジスイッチ間で最適な通信をします。もし動的なルーティングの機能が必要であれば、Vyattaを内蔵した仮想マシンを起動し、ネットワークインタフェースを複数持たせて、仮想ネットワークの間を動的にルーティングさせることもできます。

=== セキュリティグループ

エッジスイッチは各仮想マシンのトラフィック全ての関所でもあります。セキュリティグループは、この関所に、パケットの受け入れ許可ルールを指定し、仮想マシンのファイアウォールとして機能させるものです。

// TODO: 簡単な図がほしい

全ての仮想マシンをひとつずつ指定していく煩雑さを解消するため、論理名を付けたグルーピングと、グループ間の通信許可を指定することが出来るようになっています。特にグループ間の通信許可の場合は、グループに属する仮想マシンに変更があれば、相対するグループの設定にも動的に影響が及びます。OpenVNetは、このように分散したエッジスイッチの相互の影響を割り出し、常に相互の通信ルールが適切になるように制御します。

=== DHCPとDNS

DHCPやDNSなどのサービスをエッジスイッチで処理できます。

// TODO: 簡単な図がほしい

本来は、ネットワーク上にDHCPサーバを設置し、そのサーバがDHCPのディスカバリ(IPアドレスの問い合わせパケット)に応答することになっています。しかし、わざわざDHCPサーバまで到達させずとも、応答すべきパケットが自明である場合は、エッジスイッチでパケットを生成して、仮想マシンへ折り返してしまうことができます。ノードに割り振られるIPアドレスが自明である場合に、この機能が利用できます。

=== 既存ネットワークと仮想ネットワークの接続

仮想ネットワークの世界の境界(VNetEdgeと呼びます)を外界と接続するための橋渡し方式を提供します。

// TODO: 簡単な図がほしい

仮想ネットワークは、最初はどこにも接続されていないスイッチのように振る舞い、閉じたネットワークとして作成されます。物理ネットワーク上にオーバレイされた、新しい仮想ネットワークですので、既存のネットワークからどのようにしてパケットを送受信しあうかも重要なポイントになります。VNetEdgeで受け取ったパケットを読み取り、仮想ネットワークへ流し込むルールを決めるトランスレーションと言う方法があります。トランスレーションは、パケットに記載されている情報を元にした条件を記述することで、条件にマッチしたパケットを指定された仮想ネットワークへと転送するものです。例えば、特定のTagged VLANのIDを持ったパケットを、任意の仮想ネットワークへ転送してみたり、特定のIPアドレスから送られてきたパケットを、任意の仮想ネットワークのIPアドレスへ転送しNATのようにしてみたりできます。

// ------------------------------------------------------------------

== 使ってみる

OpenVNetの利用はとても簡単です。まずは、CentOSが稼働する1台のマシンにOpenVNetの全てのサービスをインストールし、使い初めてみましょう。
マシンは、物理マシンでも仮想マシンでも構いません。要件は以下の2つだけです。

- CentOS 6.6以上が稼働するマシン
- インターネット接続

[[openvnet_installation_overview]]
image::openvnet_installation_overview.png[caption="図17-1",title="1台のマシンで動作するOpenVNet環境"]

=== インストールしてみる

OpenVNetのインストールと初期設定は、以下の手順で進んでいきます。

. OpenVNetのインストール
. Redis、MySQLのインストール
. Open vSwitchの仮想ブリッジ設定
. 各種サービスの起動

それでは、この順序に沿ってOpenVNetをインストールしてみましょう。

==== OpenVNetのインストール

`openvnet.repo` をダウンロードし、 `/etc/yum/repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet.repo
```

次に、 `openvnet-third-party.repo` をダウンロードし、 `/etc/yum.repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet-third-party.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet-third-party.repo
```

それぞれのリポジトリは、以下のパッケージを含んでいます。

* `openvnet.repo`
** `openvnet`
** `openvnet-common`
** `openvnet-vna`
** `openvnet-vnmgr`
** `openvnet-webapi`
** `openvnet-vnctl`
* `openvnet-third-party.repo`
** `openvnet-ruby`
** `openvswitch`

`openvnet` パッケージはメタパッケージで、 `openvnet-common` 、 `openvnet-vna` 、 `openvnet-vnmgr` 、 `openvnet-webapi` 、および `openvnet-vnctl` パッケージに依存しています。一度に全てをインストールするために便利なパッケージです。

なお、OpenVNetのインストールには `epel` が必要ですので、 `epel-release` パッケージをインストールしておきます。

```
$ sudo yum install -y epel-release
```

ここまでが完了したら、OpenVNetパッケージをインストールします。

```
$ sudo yum install -y openvnet
```


==== Redis、MySQLのインストール

RedisおよびMySQL serverパッケージをインストールします。RedisはOpenVNetのプロセス間通信に必要で、MySQLはネットワーク構成情報を保持する為に利用されます。これらは両方必要とされていますが、OpenVNetは分散型のソフトウェアであるため、OpenVNetパッケージがこれらに依存する形にはなっていません。商用環境では、OpenVNetのプロセス群が動作するマシンとは異なるマシンにインストールされる形態を採用すると良いでしょう。

```
$ sudo yum install -y mysql-server redis
```

==== Open vSwitchの仮想ブリッジ設定

Open vSwitchを使って、 `br0` という名前の仮想ブリッジを作成します。後の疎通確認では、 `inst1` および `inst2` という2つのLXCコンテナをこのブリッジに接続します。 `br0` の設定ファイルとして、 `/etc/sysconfig/network-scripts/ifcfg-br0` を、以下の内容で作成します。

```
DEVICE=br0
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
BOOTPROTO=static
HOTPLUG=no
OVS_EXTRA="
 set bridge     ${DEVICE} protocols=OpenFlow10,OpenFlow12,OpenFlow13 --
 set bridge     ${DEVICE} other_config:disable-in-band=true --
 set bridge     ${DEVICE} other-config:datapath-id=0000aaaaaaaaaaaa --
 set bridge     ${DEVICE} other-config:hwaddr=02:01:00:00:00:01 --
 set-fail-mode  ${DEVICE} standalone --
 set-controller ${DEVICE} tcp:127.0.0.1:6633
"
```

なお、この設定では `datapath-id` を `0000aaaaaaaaaaaa` という値に設定していますが、この値はOpenVNetがブリッジを認識するための一意な識別子です。この値には16進数の値を設定できますが、後ほど利用する値ですので、憶えておいて下さい。

==== 各種サービスの起動

`openvswitch` サービスの起動と、仮想ブリッジの起動を行います。

```
$ sudo service openvswitch start
$ sudo ifup br0
```

ネットワーク構成情報を保持するデータベースとしてインストールした、MySQL serverを起動します。

```
$ sudo service mysqld start
```

OpenVNetは、OpenVNet自身に内包されたRubyを利用しますので、環境変数PATHにそのパスを設定しておく必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

Rubyにパスを通したら、データベースの作成を行います。

```
$ cd /opt/axsh/openvnet/vnet
$ bundle exec rake db:create
$ bundle exec rake db:init
```

先程述べたように、OpenVNetの各サービスはRedisで通信しますので、Redisを起動します。

```
$ service redis start
```

次に、OpenVNetのサービス群( `vnmgr` 、 `webapi` 、 `vna` )を起動します。これらを起動すると、 `/var/log/openvnet` ディレクトリにログが出力されます。もしうまく動作しない場合、このログの中に有用なエラーメッセージを見つけられる可能性があります。それでは、vnmgrとwebapiを起動してみましょう。

```
$ sudo initctl start vnet-vnmgr
$ sudo initctl start vnet-webapi
```

続いて、データベースのレコードを作成するのは、 `vnctl` ユーティリティを使用します。 `vnctl` は `openvnet-vnctl` パッケージに含まれる、WebAPIのクライアントです。先程、仮想ブリッジの作成を行った際に設定した `datapath-id` の値を憶えているでしょうか？次のコマンドで、 `vna` がどの `datapath` を管理すればよいかをOpenVNetに教えます。

```
$ vnctl datapaths add --uuid dp-test1 --display-name test1 --dpid 0x0000aaaaaaaaaaaa --node-id vna
```

`vna` がどの `datapath` を管理すれば良いかの紐付けを行ったら、 `vna` を起動してみましょう。

```
$ sudo initctl start vnet-vna
```

`ovs-vsctl` コマンドで、 `vna` が正しく動作しているかを確認することができます。

```
$ ovs-vsctl show
```

ここで、 `is_connected: true` の文字列が見えていれば、 `vna` は正しく動作しています。もしこの文字列が見えない場合、数秒待ってから再施行してみて下さい。それでも見えない場合、 `/var/log/openvnet/vna.log` を確認し、何か問題が起こっていないかを確認して下さい。

```
fbe23184-7f14-46cb-857b-3abf6153a6d6
    Bridge "br0"
        Controller "tcp:127.0.0.1:6633"
            is_connected: true
```

ここまででOpenVNetのインストールと設定は完了し、動作を開始しましたが、まだOpenVNetの仮想ネットワークに接続する仮想マシンが作成されていません。そこで、次にゲストとして2つのLXCコンテナ( `inst1` と `inst2` )を作成し、OpenVNetの仮想ネットワークに接続してみます。どのような仮想化技術でも動作はしますが、今回は、軽量かつ仮想マシン内にも簡単に構築できるLXCをインストールし、利用することにします。

```
$ sudo yum -y install lxc lxc-templates
```

`lxc` および `lxc-templates` パッケージのインストールが完了したら、コンテナのリソース制御を行う `cgroup` の利用準備を行います。

```
$ sudo mkdir /cgroup
$ echo "cgroup /cgroup cgroup defaults 0 0" >> /etc/fstab
$ sudo mount /cgroup
```

また、 `rsync` が必要になりますので、もしインストールされていない場合、以下のコマンドでrsyncをインストールして下さい。

```
$ sudo yum install -y rsync
```

LXCの動作の準備が出来ましたので、いよいよゲストの作成に入ります。

```
$ sudo lxc-create -t centos -n inst1
$ sudo lxc-create -t centos -n inst2
```

`lxc-create` を実行すると、それぞれのゲストの `root` ユーザのパスワードがどこを見れば判るかが出力されます。このパスワードは後でゲストにログインする際に利用しますので、憶えておいて下さい。次に、ゲストのネットワークインタフェースの設定を行います。 `/var/lib/lxc/inst1/config` ファイルを開き、内容を以下で置き換えて下さい。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst1
lxc.network.hwaddr = 10:54:FF:00:00:01
lxc.rootfs = /var/lib/lxc/inst1/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst1
lxc.autodev = 0
```

同様に、 `/var/lib/lxc/inst2/config` ファイルを開き、内容を以下で置き換えます。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst2
lxc.network.hwaddr = 10:54:FF:00:00:02
lxc.rootfs = /var/lib/lxc/inst2/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst2
lxc.autodev = 0
```

注意点として、 今回はLinux BridgeがOpen vSwitchで置き換えられているため、 Linux Bridgeにネットワークインタフェースが設定されることを前提とした `lxc.network.link` パラメータは使用しません。その代わりに、この後に仮想ブリッジに手動でインタフェースを設定します。設定ファイルの内容を置き換えたら、LXCコンテナを起動します。

```
$ sudo lxc-start -d -n inst1
$ sudo lxc-start -d -n inst2
```

LXCコンテナが起動したら、先述したとおり、起動したコンテナのネットワークインタフェースを先程設定した仮想ブリッジに手動で接続します。これは、基本的にネットワークのケーブルを物理スイッチに挿入するのと同じです。

```
$ sudo ovs-vsctl add-port br0 inst1
$ sudo ovs-vsctl add-port br0 inst2
```

これで、OpenVNetのインストールと、OpenVNetの仮想ネットワークを体験する準備が整いました。次の節では、最も基本的な1つの仮想ネットワークセグメントの作成を試してみます。

=== CLIで操作してみる

仮想ネットワークの作成などの操作は、前節でも登場した `vnctl` で行うことが出来ます。まずは、1つの仮想ネットワークセグメントを作成してみましょう。

[[openvnet_cli_simplenetwork]]
image::openvnet_cli_simplenetwork.png[caption="図17-1",title="最も基本的な1つの仮想ネットワークセグメント"]

作成する仮想ネットワークのアドレスを `10.100.0.0/24.` とし、 inst1` のIPアドレスを `10.100.0.10`、`inst2` のIPアドレスを `10.100.0.11`とします。それでは、 `vnctl` コマンドを使用して仮想ネットワークを作成してみます。 `vnctl` コマンドで作成する対象は、 `networks` です。

```
$ vnctl networks add \
  --uuid nw-test1 \
  --display-name testnet1 \
  --ipv4-network 10.100.0.0 \
  --ipv4-prefix 24 \
  --network-mode virtual
```

この1つのコマンドだけで、仮想ネットワークが作成されました。次に、どのIPアドレスを持つどのネットワークインタフェースが、その仮想ネットワークに所属しているのかを `vnctl` コマンドでOpenVNetに教えます。 操作する対象は、 `interfaces` です。まずは、 `inst1` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst1 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:01 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.10 \
  --port-name inst1
```

同様に、 `inst2` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst2 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:02 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.11 \
  --port-name inst2
```

この操作により、OpenVNetは `10.100.0.0/24` の仮想ネットワークを作成し、そこにそれぞれ `10.100.0.10` 、 `10.100.0.11` のIPアドレスを持つネットワークインタフェースが接続されていることを定義しました。

=== 疎通確認をする

最後に、2つのゲストが仮想ネットワークを通じて疎通ができることを確認します。まず `inst1` にログインし、IPアドレスを確認してみます。

```
$ lxc-console -n inst1
$ ip a
```

この操作時点ではまだ `inst1` の `eth0` にIPアドレスを付与していないため、IPアドレスが表示されませんが、これは正しい動作です。
先程作成した仮想ネットワークはDHCPサービスを有効にしていないため、IPアドレスは手動で付与する必要があります。

それでは、`inst1` の `eth0` にIPアドレスを付与します。付与するIPアドレスは、`vnctl` で `inst1` のインタフェースのIPアドレスとして設定した `10.100.0.10` です。

```
$ ip addr add 10.100.0.10/24 dev eth0
```

もう1つ端末を開き、 `inst2` に対し同じ操作を行います。ここで `inst2` の `eth0` に付与するIPアドレスは、 `10.100.0.11` です。

```
$ lxc-console -n inst2
$ ip addr add 10.100.0.11/24 dev eth0
```

これで2つのゲストに仮想ネットワーク内のIPアドレスが付与されたので、お互いに `ping` を実行してみます。まずは、 `inst2` から `inst1` に `ping` を実行します。

```
$ ping 10.100.0.10
```

うまく行った場合、pingは正しく動作し、疎通が確認できるはずです。もしうまく動作しない場合は、ここまでの手順で誤りがなかったかを確認してみて下さい。
疎通ができるようになったところで、注目すべき点として、従来のネットワークとOpenVNetの仮想ネットワークとの違いを1つ紹介します。

先程 `inst2` の `eth0` に設定したIPアドレスを、 `10.100.0.11/24` から `10.100.0.15/24` に変更してみましょう。

```
$ sudo ip addr del 10.100.0.11/24 dev eth0
$ sudo ip addr add 10.100.0.15/24 dev eth0
```

設定が終わったら、また `inst1` に対して `ping` を実行してみます。

```
$ ping 10.100.0.10
```

うまく動作したでしょうか？先程とは異なり、疎通ができなくなったことが確認できるはずです。これがもし従来のネットワークだった場合、 `10.100.0.0/24` の範囲内のIPアドレスに変更したとしても疎通できますが、OpenVNetはデータベースに従ってより厳格に制限を行うため、`inst2` のIPアドレスが `10.100.0.11` でない限り、通信を許可しません。

=== フローの変化を見る

OpenVNetはOpenFlowで仮想ネットワークをコントロールしていますが、フローエントリを `ovs-ofctl` でそのまま確認するのは大変です。
OpenVNetには `vna` と共にインストールされる `vnflows-monitor` というツールが付属しており、
フロー制御の節で解説したOpenVNetのフローテーブルの分類に基づいて、現在のOpen vSwitchのフローエントリを読みやすく整形して表示してくれます。

`vnflows-monitor` を実行するには、OpenVNetが内包するRubyにパスが通っている必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

それでは、 `vnflows-monitor` でフローエントリを表示してみましょう。

```
$ cd /opt/axsh/openvnet/vnet/bin/
$ ./vnflows-monitor
```

Open vSwitchが正しく動作していて、フローエントリが存在する場合、例として以下のような内容が表示されます。

```
(0): TABLE_CLASSIFIER
  0-00        0       0 => SWITCH(0x0)               actions=write_metadata:REMOTE(0x0),goto_table:TABLE_TUNNEL_PORTS(3)
  0-01        0       0 => SWITCH(0x0)              tun_id=0 actions=drop
  0-02       28       0 => PORT(0x1)                in_port=1 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x1),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02       22       0 => PORT(0x2)                in_port=2 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x5),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02        0       0 => SWITCH(0x0)              in_port=CONTROLLER actions=write_metadata:LOCAL|NO_CONTROLLER(0x0),goto_table:TABLE_CONTROLLER_PORT(7)
  0-02        0       0 => PORT(0x7ffffffe)         in_port=LOCAL actions=write_metadata:LOCAL(0x0),goto_table:TABLE_LOCAL_PORT(6)
(3): TABLE_TUNNEL_PORTS
  3-00        0       0 => SWITCH(0x0)               actions=drop
(4): TABLE_TUNNEL_NETWORK_IDS
  4-00        0       0 => SWITCH(0x0)               actions=drop
  4-30        0       0 => ROUTE_LINK(0x1)          tun_id=0x10000001,dl_dst=02:00:10:00:00:01 actions=write_metadata:TYPE_ROUTE_LINK(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
  4-30        0       0 => NETWORK(0x1)             tun_id=0x80000001 actions=write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
  4-30        0       0 => NETWORK(0x2)             tun_id=0x80000002 actions=write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
(6): TABLE_LOCAL_PORT
  6-00        0       0 => SWITCH(0x0)               actions=drop
...
```

このように、フローエントリが (0): TABLE_CLASSIFIER といった形で、OpenVNetのフローテーブルの分類でグループ化されて表示されます。
また、それぞれのフローテーブルの下に表示される行の意味は、左から順に、以下のようになっています。

. フローエントリの優先度に従ったフローテーブルのインデックス
. そのフローエントリにマッチしたパケット数
. フローの `cookie`
. フローの `match`
. フローの `action`

なお、`vnflows-monitor` には、フローの継続的な監視を行う機能もあります。これは `vnflows-monitor` の最も有用な特徴の1つであり、フローエントリの変化がすぐに画面出力に反映されます。この機能を利用するには、 `vnflows-monitor` に以下のような引数を付加して起動します。

```
$ cd /opt/axsh/openvnet/vnet/bin
$ ./vnflows-monitor -d -c 0
```

この方法で起動すると、最初は何も表示されず、パケットが流れるのを待機している状態になります。
この状態で、例として、先程の `inst1` と `inst2` の間で `ping` を実行した時には、次のような内容が出力されます。

```
-------run:4--iteration:43-------
(0): TABLE_CLASSIFIER
  0-02       34       0 => PORT(0x1)                in_port=1 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x1),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02       28       0 => PORT(0x2)                in_port=2 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x5),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
(15): TABLE_INTERFACE_EGRESS_CLASSIFIER
 15-30       11       0 => INTERFACE(0x1)[0x12]     ip,metadata=TYPE_INTERFACE(0x1),dl_src=10:54:ff:00:00:01,nw_src=10.100.0.10 actions=write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_INTERFACE_EGRESS_FILTER(18)
 15-30        8       0 => INTERFACE(0x5)[0x12]     ip,metadata=TYPE_INTERFACE(0x5),dl_src=10:54:ff:00:00:02,nw_src=192.168.50.10 actions=write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_INTERFACE_EGRESS_FILTER(18)
(18): TABLE_INTERFACE_EGRESS_FILTER
 18-00       38       0 => SWITCH(0x0)               actions=goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
(20): TABLE_NETWORK_SRC_CLASSIFIER
 20-30       25       0 => NETWORK(0x1)             metadata=TYPE_NETWORK(0x1) actions=goto_table:TABLE_ROUTE_INGRESS_INTERFACE(30)
 20-30       13       0 => NETWORK(0x2)             metadata=TYPE_NETWORK(0x2) actions=goto_table:TABLE_ROUTE_INGRESS_INTERFACE(30)
(30): TABLE_ROUTE_INGRESS_INTERFACE
 30-10        8       0 => INTERFACE(0x6)[0x12]     ip,metadata=TYPE_NETWORK(0x1),dl_dst=02:00:00:00:02:01 actions=write_metadata:TYPE_INTERFACE(0x6),goto_table:TABLE_ROUTE_INGRESS_TRANSLATION(31)
 30-10        8       0 => INTERFACE(0x7)[0x12]     ip,metadata=TYPE_NETWORK(0x2),dl_dst=02:00:00:00:02:02 actions=write_metadata:TYPE_INTERFACE(0x7),goto_table:TABLE_ROUTE_INGRESS_TRANSLATION(31)
(31): TABLE_ROUTE_INGRESS_TRANSLATION
 31-90        8       0 => INTERFACE(0x6)           metadata=TYPE_INTERFACE(0x6) actions=goto_table:TABLE_ROUTER_INGRESS_LOOKUP(32)
 31-90        8       0 => INTERFACE(0x7)           metadata=TYPE_INTERFACE(0x7) actions=goto_table:TABLE_ROUTER_INGRESS_LOOKUP(32)
(32): TABLE_ROUTER_INGRESS_LOOKUP
 32-30        8       0 => ROUTE(0x1)               ip,metadata=TYPE_INTERFACE(0x6),nw_src=10.100.0.0/24 actions=write_metadata:TYPE_ROUTE_LINK|REFLECTION(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
 32-30        8       0 => ROUTE(0x2)               ip,metadata=TYPE_INTERFACE(0x7),nw_src=192.168.50.0/24 actions=write_metadata:TYPE_ROUTE_LINK|REFLECTION(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
(33): TABLE_ROUTER_CLASSIFIER
 33-30       16       0 => ROUTE_LINK(0x1)          metadata=TYPE_ROUTE_LINK(0x1) actions=goto_table:TABLE_ROUTER_EGRESS_LOOKUP(34)
(34): TABLE_ROUTER_EGRESS_LOOKUP
 34-30        8       0 => ROUTE(0x1)               ip,metadata=TYPE_ROUTE_LINK(0x1),nw_dst=10.100.0.0/24 actions=write_metadata:0x8000000600000001,goto_table:TABLE_ROUTE_EGRESS_LOOKUP(35)
 34-30        8       0 => ROUTE(0x2)               ip,metadata=TYPE_ROUTE_LINK(0x1),nw_dst=192.168.50.0/24 actions=write_metadata:0x8000000700000001,goto_table:TABLE_ROUTE_EGRESS_LOOKUP(35)
(35): TABLE_ROUTE_EGRESS_LOOKUP
 35-20        8       0 => INTERFACE(0x6)[0x12]     metadata=VALUE_PAIR(0x8000000600000000/0xffffffff00000000)(0x0) actions=write_metadata:0x702000000000006,goto_table:TABLE_ROUTE_EGRESS_TRANSLATION(36)
 35-20        8       0 => INTERFACE(0x7)[0x12]     metadata=VALUE_PAIR(0x8000000700000000/0xffffffff00000000)(0x0) actions=write_metadata:0x702000000000007,goto_table:TABLE_ROUTE_EGRESS_TRANSLATION(36)
(36): TABLE_ROUTE_EGRESS_TRANSLATION
 36-90        8       0 => INTERFACE(0x6)           metadata=TYPE_INTERFACE(0x6) actions=goto_table:TABLE_ROUTE_EGRESS_INTERFACE(37)
 36-90        8       0 => INTERFACE(0x7)           metadata=TYPE_INTERFACE(0x7) actions=goto_table:TABLE_ROUTE_EGRESS_INTERFACE(37)
(37): TABLE_ROUTE_EGRESS_INTERFACE
 37-20        8       0 => INTERFACE(0x6)[0x12]     metadata=TYPE_INTERFACE(0x6) actions=set_field:02:00:00:00:02:01->eth_src,write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_ARP_TABLE(40)
 37-20        8       0 => INTERFACE(0x7)[0x12]     metadata=TYPE_INTERFACE(0x7) actions=set_field:02:00:00:00:02:02->eth_src,write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_ARP_TABLE(40)
(40): TABLE_ARP_TABLE
 40-40        8       0 => INTERFACE(0x1)[0x12]     ip,metadata=TYPE_NETWORK(0x1),nw_dst=10.100.0.10 actions=set_field:10:54:ff:00:00:01->eth_dst,goto_table:TABLE_NETWORK_DST_CLASSIFIER(42)
 40-40        8       0 => INTERFACE(0x5)[0x12]     ip,metadata=TYPE_NETWORK(0x2),nw_dst=192.168.50.10 actions=set_field:10:54:ff:00:00:02->eth_dst,goto_table:TABLE_NETWORK_DST_CLASSIFIER(42)
(42): TABLE_NETWORK_DST_CLASSIFIER
 42-30       25       0 => NETWORK(0x1)             metadata=TYPE_NETWORK(0x1) actions=goto_table:TABLE_NETWORK_DST_MAC_LOOKUP(43)
 42-30       13       0 => NETWORK(0x2)             metadata=TYPE_NETWORK(0x2) actions=goto_table:TABLE_NETWORK_DST_MAC_LOOKUP(43)
(43): TABLE_NETWORK_DST_MAC_LOOKUP
 43-60       12       0 => INTERFACE(0x1)[0x12]     metadata=TYPE_NETWORK(0x1),dl_dst=10:54:ff:00:00:01 actions=write_metadata:TYPE_INTERFACE(0x1),goto_table:TABLE_INTERFACE_INGRESS_FILTER(45)
 43-60        8       0 => INTERFACE(0x5)[0x12]     metadata=TYPE_NETWORK(0x2),dl_dst=10:54:ff:00:00:02 actions=write_metadata:TYPE_INTERFACE(0x5),goto_table:TABLE_INTERFACE_INGRESS_FILTER(45)
(45): TABLE_INTERFACE_INGRESS_FILTER
 45-90       11       0 => INTERFACE(0x1)[0x71]     metadata=TYPE_INTERFACE(0x1) actions=goto_table:TABLE_OUT_PORT_INTERFACE_INGRESS(90)
 45-90        8       0 => INTERFACE(0x5)[0x71]     metadata=TYPE_INTERFACE(0x5) actions=goto_table:TABLE_OUT_PORT_INTERFACE_INGRESS(90)
(90): TABLE_OUT_PORT_INTERFACE_INGRESS
 90-10       12       0 => PORT(0x1)                metadata=TYPE_INTERFACE(0x1) actions=output:1
 90-10        8       0 => PORT(0x2)                metadata=TYPE_INTERFACE(0x5) actions=output:2
```

`inst1` と `inst2` の間でICMP Echo RequestとICMP Echo ReplyがOpen vSwitchの仮想ブリッジを横断すると、マッチした全てのフローエントリが表示され、マッチしたパケット数のカウンタが増加していきます。
この機能により、パケットがOpen vSwitchのどのフローエントリを通過して処理されたかを、一目で知ることができます。また、他の使い方として、例えば `vnctl` で仮想ネットワークを操作した時に、どのようなフローエントリが追加、あるいは削除されたかも確認することができます。

== OpenVNetの活用例

OpenVNetはすでに活用が始まっています。たとえば、京セラコミュニケーションシステムやTIS株式会社にて、OpenFlowの実案件活用や仮想ネットワークの実証実験などを行っており、OpenVNetのテクノロジが活躍しています。

=== 分散するDockerを仮想L2で連結する

次に２つ目の活用例として、複数のサーバ上に分散するDockerコンテナをOpenVNetを用いた仮想L2セグメントで連結する方法をみていきましょう。

これは2015年のはじめに、TIS株式会社が仮想ネットワークとコンテナ技術の実験を行った事例となります。

=== Dockerのネットワーク

Docker footnote:[Dockerの詳細は、Dockerの公式ドキュメント(https://docs.docker.com/)を参照ください] とは、dotCloud社（現Docker社）が自社のパブリックPaaSを実現するために開発した技術をOSS化したものです。アプリケーションの実行環境を容易に素早く、かつ他の影響を受けないようにして立ち上げるために、Dockerは様々なLinuxの技術を用いて「他から隔離された環境（＝コンテナ） 」を作り出します。

Dockerは様々なリソースを隔離しますが、ネットワークもその隔離すべきリソースの一つです。そのためDockerは、Network Namespaceや仮想NIC等の技術を用いて、ホストとなるLinuxサーバ上に他から隔離された内部ネットワークを構成します。ただしそのままではサーバの外部と通信ができませんので、Dockerは通常、IPマスカレードとポートフォワードをiptablesに設定することで外部ネットワークと連携できるようにします。

[[docker_network]]
image::docker_network.png[caption="図17-1",title="Dockerのネットワーク"]

単独のサーバ内でDockerを利用するだけならこの方式で良いのですが、複数のサーバでDockerを動作させたい場合には問題が生じます。Dockerコンテナが所属するネットワークはサーバ内に閉じていますので、異なるサーバで動作しているDockerコンテナ同士が、そのDockerコンテナに付与されたIPアドレスで通信することができないのです。

この問題を解決するために、 coreos/flannel footnote:[https://github.com/coreos/flannel] や weaveworks/weave footnote:[https://github.com/weaveworks/weave] 等の様々なDockerネットワーキングツールが公開されています。また2015年10月にリリースされたDocker v1.9からは、Docker自身が複数サーバを跨った仮想ネットワークを構成できるようになりました。

しかしこの実験を行った時点ではまだ、Docker自身は仮想ネットワークを構成する機能を持っていませんでした。また coreos/flannel や weaveworks/weave といったツールには、セキュリティグループのようなOpenVNetが持つ高度なネットワーク機能がありませんでした。そこで本実験では、OpenVNetを用いて敷設した仮想L2セグメントにDockerコンテナを所属させることで、サーバを跨ったDockerコンテナ間がシームレスに通信できること、及びDockerのネットワークにセキュリティグループのような高度なネットワーク機能を付与できることを確認しました。

=== Docker+OpenVNet

同一サブネット内のサーバ2台と、ルータを挟んだ別のサブネットにあるサーバ1台の、合計3つのサーバ上でDockerコンテナを動作させ、それらをOpenVNetを用いて敷設した仮想L2セグメントに所属させてみましょう。

まずは各サーバ上でDockerコンテナを立ち上げた後、DockerコンテナのNetwork Namespaceとopen vSwitchにvethペアを放り込みます。この際、後からOpenVNetに設定できるように、Dockerコンテナ側のvethのMACアドレスとIPアドレスを明示的に指定しておきます。

次にOpenVNetを設定します。Open vSwitchのdatapathIDを重複しないように指定し、各サーバが所属する物理ネットワークとOpenVNetが敷設する仮想L2セグメントをOpenVNetに定義します（この際、異なる物理サブネットに存在するサーバ間はGREトンネルが自動的に敷設されます）。各サーバの物理NICと、Dockerコンテナにvethペアとして放り込んだ仮想NICの情報、及びセキュリティグループをOpenVNetに設定した後に、各仮想NICにセキュリティグループを割り当て、OpenVNet上に仮想ルータを構成して物理ネットワークと仮想L2セグメント間のルーティングを定義しましょう。

最後に各サーバとDockerコンテナにスタティックルートを設定すれば、OpenVNetを用いたDockerネットワーキングが完成します。各サーバ上のDockerコンテナはOpenVNetが敷設した同じ仮想L2セグメントに所属していますので、異なるサーバのDockerコンテナへそのIPアドレスを用いて通信することが可能となります。またセキュリティグループの設定に従い、到達すべきでないパケットはOpenVNetがDROPするため、個々のDockerコンテナにパケットフィルタルールを定義する必要が無くなります。

image::docker_openvnet_1.png[caption="図17-2",title="OpenVNetを用いたDockerネットワーキング"]

なお、ここで説明した手順を実際に実行し動作させるツールキットを、walfisch footnote:[https://github.com/tech-sketch/walfisch] というオープンソースソフトウェアとして公開しています。実際に実行したコマンドが標準出力に表示されますので、OpenVNetを用いたDockerネットワーキングに興味がある方は一度動作させてみると良いでしょう。

=== 分散するデータセンタ間を仮想L2で連結する

最後に、 複数のデータセンタ間を跨って任意の仮想L2ネットワークを構成する例を見てみましょう。
この例は、2014年度にTIS株式会社と株式会社あくしゅが協力し、各IaaSやオンプレミスのネットワーク機能に依存しないネットワーク制御について、OpenVNetを活用して共同検証を行ったものです。

現存するパブリックIaaSの持つネットワーク機能は、それぞれ大きく利用方法や特徴が異なっています。このため、パブリックIaaSの利用者はそれらに強く依存したシステム設計を行う必要があります。しかし、OpenVNetを利用することで、パブリックIaaSのネットワーク機能に依存せず、複数のパブリックあるいはプライベートIaaSに跨った仮想的なL2ネットワークを構成することが可能となるため、IaaS間の段階的なシステム移行の実現性を高めることができます。

=== プライベートIaaSとパブリックIaaSの連結例

それでは、プライベートIaaSとパブリックIaaSのL2ネットワークを仮想的に連結する構成例をみてみましょう。

OpenVNetは独立して動作することができますが、本来は仮想データセンタを構築するOSSであるwakame-vdc footnote:[https://github.com/axsh/wakame-vdc] のネットワーク機能としてスピンアウトしたソフトウェアであるため、プライベートIaaSとしてwakame-vdc、パブリックIaaSとしてAmazon Web Servicesを利用するケースを想定します。

[[narukozaka_tools]]
image::narukozaka_tools.png[caption="図17-4",title="プライベートIaaSとパブリックIaaSの連結構成"]

OpenVNetは、フローによってOpenVNetの仮想ネットワークと外部のネットワークの間をシームレスに接続するVNetEdge機能を持っています。

この構成例では、仮想ネットワークIDとVLAN IDの変換規則をOpenVNetに登録しておくことで、wakame-vdcの仮想ネットワークと、Amazon Web ServicesのVirtual Private Cloudで構築されたネットワークの間を流れるパケットがVNetEdgeのOpen vSwitchを通過する際に、この２つのネットワークが同一のL2ネットワークであるかのようにパケット転送を制御します。

このツールキットはOSSとして公開しており footnote:[https://github.com/cloudconductor-incubator/narukozaka-tools]、この他にも多くの機能を持ちます。

* IaaSのインスタンスイメージの作成と起動
* IaaSのインスタンスにインストールするミドルウェアの自動設定
* IaaSのネットワーク上に、VNetEdgeをスイッチとしたスター型のネットワークトポロジを構築する機能
* wakame-vdcとパブリックIaaSの間を自動的にトンネリングする機能

またセキュリティの案件に応じ、wakame-vdc側のインスタンスとIaaS側のVNetEdge間のGREトンネリングを暗号化するといった、柔軟な対応も可能です。

== まとめ

* OpenVNetはオープンソースライセンスLGPL3に基づくフリーソフトウェアであり、バザール式のオープンな開発コミュニティを持っている
* エッジオーバレイ仮想ネットワークを実現できるため、物理ネットワークへの影響がほとんど無い
* オンプレミス環境以外にも、AWSに代表されるパブリッククラウドでも利用ができる
* 仮想マシンだけでなく、Dockerに代表されるコンテナが主体の基盤とも組み合わせて利用できる

最後に、OpenVNetは、Trema同様に、常時開発にご協力いただける方々を募集しております。腕に覚えのある方は、ぜひ下記の情報をご参照の上、奮ってご参加いただければ幸いです。

【URL】 http://openvnet.org/
[[wesite_openvnet]]
image::QR_to_openvnet.gif[caption="図xx-y",title="OpenVNetのサイトへのリンク"]


